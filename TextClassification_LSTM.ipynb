{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ref - https://www.kaggle.com/marcovasquez/basic-nlp-with-tensorflow-and-wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rajku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Natural Language Tool Kit \n",
    "import nltk  \n",
    "nltk.download('stopwords') \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.porter import PorterStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test.csv')\n",
    "submission = pd.read_csv('./sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardModel(nn.Module):\n",
    "    def __init__(self, embedding_matrix, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag.from_pretrained(torch.FloatTensor(embedding_matrix))\n",
    "        self.fc1 = nn.Linear(embed_dim, 10)\n",
    "        self.fc2 = nn.Linear(10, 1)\n",
    "        self.output = nn.Sigmoid()\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, input, offsets):\n",
    "        #print(input)\n",
    "        embedded = self.embedding(input, offsets)\n",
    "        h1 = F.tanh(self.fc1(embedded))\n",
    "        h2 = self.fc2(h1)\n",
    "        return self.output(h2)\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.fc1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.bias.data.zero_()\n",
    "        self.fc2.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAndFeedForward(nn.Module):\n",
    "    def __init__(self, embed_weight, embed_dim, hidden_dim, num_layer):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layer = num_layer\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embed_weight))\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layer, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim, 10)\n",
    "        self.fc2 = nn.Linear(10, 1)\n",
    "        self.output = nn.Sigmoid()\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, input, offsets):\n",
    "        emb = self.embedding(input)\n",
    "        out, (hn, cn) = self.lstm(emb)\n",
    "        h1 = F.tanh(self.fc1(hn[-1]))\n",
    "        h2 = self.fc2(h1)\n",
    "        return self.output(h2)\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.fc1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc1.bias.data.zero_()\n",
    "        self.fc2.bias.data.zero_()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "\n",
    "def remove_html(text):\n",
    "    no_html= pattern.sub('',text)\n",
    "    return no_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all text that start with html\n",
    "train['text']=train['text'].apply(lambda x : remove_html(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all text that start with html in test\n",
    "test['text']=test['text'].apply(lambda x : remove_html(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(dataset):\n",
    "    corpus = []  \n",
    "    for i in range(0, len(dataset)):  \n",
    "        text = re.sub('[^a-zA-Z]', ' ', dataset['text'][i])  \n",
    "        text = text.lower()  \n",
    "        # split to array(default delimiter is \" \") \n",
    "        text = text.split()  \n",
    "        text = ' '.join(text)    \n",
    "        corpus.append(text)  \n",
    "        \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = clean_text(train)\n",
    "test['text'] = clean_text(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is base in 80% of the data, an only text and targert at this moment\n",
    "training_size = 6090\n",
    "training_sentences = train.text.values\n",
    "training_labels = train.target.values\n",
    "testing_sentences = test.text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "embedding_matrix = []\n",
    "embedding_idx_word = {}\n",
    "embedding_dim = 50\n",
    "\n",
    "embeddings_dict['unk'] = 0\n",
    "embedding_matrix.append(np.zeros(embedding_dim))\n",
    "embedding_idx_word[0] = 'unk'\n",
    "\n",
    "file = './glove.twitter.27B/glove.twitter.27B.50d.txt'\n",
    "with open(file, 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        if (len(vector) != embedding_dim):\n",
    "            continue\n",
    "        embedding_idx_word[len(embedding_idx_word)] = word\n",
    "        embeddings_dict[word] = len(embeddings_dict)\n",
    "        embedding_matrix.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterDataSet(Dataset):    \n",
    "    def __init__(self, input, labels, vocab, isFixed, size):\n",
    "        self.vocab = vocab\n",
    "        self.data = []\n",
    "        print(labels[0:22])\n",
    "        for i in range(len(input)):\n",
    "            tokens = input[i].split()\n",
    "            #print(tokens)\n",
    "            if isFixed:\n",
    "                ttokens = np.zeros(size, dtype = int)\n",
    "                for j in range(min(size, len(tokens))):\n",
    "                    #print(tokens[i])\n",
    "                    if tokens[j] in self.vocab:\n",
    "                        ttokens[j] = self.vocab[tokens[j]]\n",
    "                        #print(ttokens[i])\n",
    "                    else:\n",
    "                        ttokens[j] = self.vocab['unk']\n",
    "                    #print(ttokens)\n",
    "                tokens = ttokens\n",
    "            else:\n",
    "                tokens = [self.vocab[token] if token in self.vocab else self.vocab['unk'] for token in tokens]\n",
    "            self.data.append((labels[i], tokens))\n",
    "            #print(tokens)\n",
    "            #break\n",
    "        self.labels = labels\n",
    "        #print(self.data)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __getlabel__(self, index):\n",
    "        return this.labels[index]\n",
    "    \n",
    "                \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def getvocab(self):\n",
    "        return self.vocab\n",
    "    \n",
    "    def getTensor(self, sentence):\n",
    "        tokens = sentence.split()\n",
    "        tokens = torch.tensor([[self.vocab[token] if token in self.vocab else self.vocab['unk'] for token in tokens]])\n",
    "        return tokens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    label = torch.tensor([entry[0] * 1.0 for entry in batch])\n",
    "    text = torch.tensor([entry[1] for entry in batch])\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = text.type(torch.int64)\n",
    "    return text, offsets, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 32\n",
    "number_layer = 32\n",
    "lstm_dataset = TwitterDataSet(train.text.values, train.target.values, embeddings_dict, True, number_layer)\n",
    "lstModel = LSTMAndFeedForward(embedding_matrix, embedding_dim, hidden_dim, number_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twitter_dateSet = TwitterDataSet(train.text.values, train.target.values, embeddings_dict, False, 0)\n",
    "#ffmodel = FeedForwardModel(embedding_matrix, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fuction = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(lstModel.parameters(), lr=4.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(sub_train_, model):\n",
    "    # Train the model\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    count = 0.0\n",
    "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,collate_fn=generate_batch)\n",
    "    for i, (text, offsets, cls) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(text, offsets)\n",
    "        loss = loss_fuction(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        outputLabel = (output > 0.5).float()\n",
    "        outputLabel = outputLabel.view(-1)\n",
    "        train_acc += ((outputLabel == cls).float()).sum()\n",
    "        count = count + len(cls)\n",
    "        #print('outputLength : %d train_acc %f count %f acc %f' %(len(outputLabel), train_acc, count, train_acc/count))\n",
    "        #print('%f' %(train_acc/count))\n",
    "        #print(train_acc)\n",
    "        #print(train_acc)\n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    return train_loss/count, train_acc/count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_func(sub_valid_, model):\n",
    "    valid_loss = 0.0\n",
    "    valid_acc = 0.0\n",
    "    count = 0.0\n",
    "    data = DataLoader(sub_valid_, batch_size=BATCH_SIZE, shuffle=True,collate_fn=generate_batch)\n",
    "    for i, (text, offsets, cls) in enumerate(data):\n",
    "        output = model(text, offsets)\n",
    "        loss = loss_fuction(output, cls)\n",
    "        valid_loss += loss.item()\n",
    "        outputLabel = (output > 0.5).float()\n",
    "        outputLabel = outputLabel.view(-1)\n",
    "        valid_acc += ((outputLabel == cls).float()).sum()\n",
    "        count = count + len(cls)\n",
    "    return valid_loss/count, valid_acc/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajku\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "C:\\Users\\rajku\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\loss.py:512: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "C:\\Users\\rajku\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\loss.py:512: UserWarning: Using a target size (torch.Size([18])) that is different to the input size (torch.Size([18, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
      "C:\\Users\\rajku\\.conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\loss.py:512: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | time in 1 minutes, 23 seconds\n",
      "\tLoss: 0.1090(train)\t|\tAcc: 49.3%(train)\n",
      "\tLoss: 0.0977(valid)\t|\tAcc: 42.8%(valid)\n",
      "Epoch: 3  | time in 1 minutes, 23 seconds\n",
      "\tLoss: 0.1241(train)\t|\tAcc: 49.5%(train)\n",
      "\tLoss: 0.1364(valid)\t|\tAcc: 57.2%(valid)\n",
      "Epoch: 5  | time in 1 minutes, 22 seconds\n",
      "\tLoss: 0.0899(train)\t|\tAcc: 53.2%(train)\n",
      "\tLoss: 0.0956(valid)\t|\tAcc: 57.2%(valid)\n",
      "Epoch: 7  | time in 1 minutes, 23 seconds\n",
      "\tLoss: 0.0854(train)\t|\tAcc: 50.0%(train)\n",
      "\tLoss: 0.0346(valid)\t|\tAcc: 57.2%(valid)\n",
      "Epoch: 9  | time in 1 minutes, 21 seconds\n",
      "\tLoss: 0.0625(train)\t|\tAcc: 51.1%(train)\n",
      "\tLoss: 0.0321(valid)\t|\tAcc: 42.8%(valid)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "train_len = int(len(lstm_dataset) * 0.20)\n",
    "sub_train_, sub_valid_ = random_split(lstm_dataset, [train_len, len(lstm_dataset) - train_len])\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_func(sub_train_, lstModel)\n",
    "    valid_loss, valid_acc = valid_func(sub_valid_, lstModel)\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "    if (epoch % 2) == 0:\n",
    "        print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "        print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "        print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fun(sentence):\n",
    "    #print(\" input \" + sentence)\n",
    "    if len(sentence) == 0:\n",
    "        sentence = 'awesome'\n",
    "    tensor = lstm_dataset.getTensor(sentence)\n",
    "    t1 = lstModel(tensor, torch.tensor([0]))\n",
    "    t1 = t1.view(-1)\n",
    "    return t1[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"modelPredict\"] = train['text'].apply(lambda x : predict_fun(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"modelDebugging.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['target'] = test['text'].apply(lambda x : predict_fun(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = (test['target'] > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3258</td>\n",
       "      <td>10861</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3259</td>\n",
       "      <td>10865</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3260</td>\n",
       "      <td>10868</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3261</td>\n",
       "      <td>10874</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3262</td>\n",
       "      <td>10875</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  target\n",
       "0         0       1\n",
       "1         2       1\n",
       "2         3       1\n",
       "3         9       1\n",
       "4        11       1\n",
       "...     ...     ...\n",
       "3258  10861       1\n",
       "3259  10865       1\n",
       "3260  10868       1\n",
       "3261  10874       1\n",
       "3262  10875       1\n",
       "\n",
       "[3263 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just happened a terrible car crash</td>\n",
       "      <td>0.917844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>heard about earthquake is different cities sta...</td>\n",
       "      <td>0.917802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond geese are ...</td>\n",
       "      <td>0.917791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>apocalypse lighting spokane wildfires</td>\n",
       "      <td>0.917981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>typhoon soudelor kills in china and taiwan</td>\n",
       "      <td>0.917821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3258</td>\n",
       "      <td>10861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>earthquake safety los angeles safety fasteners...</td>\n",
       "      <td>0.917821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3259</td>\n",
       "      <td>10865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>storm in ri worse than last hurricane my city ...</td>\n",
       "      <td>0.917791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3260</td>\n",
       "      <td>10868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>green line derailment in chicago</td>\n",
       "      <td>0.917888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3261</td>\n",
       "      <td>10874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>meg issues hazardous weather outlook hwo</td>\n",
       "      <td>0.917844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3262</td>\n",
       "      <td>10875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cityofcalgary has activated its municipal emer...</td>\n",
       "      <td>0.917809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         0     NaN      NaN   \n",
       "1         2     NaN      NaN   \n",
       "2         3     NaN      NaN   \n",
       "3         9     NaN      NaN   \n",
       "4        11     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "3258  10861     NaN      NaN   \n",
       "3259  10865     NaN      NaN   \n",
       "3260  10868     NaN      NaN   \n",
       "3261  10874     NaN      NaN   \n",
       "3262  10875     NaN      NaN   \n",
       "\n",
       "                                                   text    target  \n",
       "0                    just happened a terrible car crash  0.917844  \n",
       "1     heard about earthquake is different cities sta...  0.917802  \n",
       "2     there is a forest fire at spot pond geese are ...  0.917791  \n",
       "3                 apocalypse lighting spokane wildfires  0.917981  \n",
       "4            typhoon soudelor kills in china and taiwan  0.917821  \n",
       "...                                                 ...       ...  \n",
       "3258  earthquake safety los angeles safety fasteners...  0.917821  \n",
       "3259  storm in ri worse than last hurricane my city ...  0.917791  \n",
       "3260                   green line derailment in chicago  0.917888  \n",
       "3261           meg issues hazardous weather outlook hwo  0.917844  \n",
       "3262  cityofcalgary has activated its municipal emer...  0.917809  \n",
       "\n",
       "[3263 rows x 5 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
